{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Switch Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from environment_gym import TrafficEnvironment\n",
    "from task2_environment import CustomTrafficEnvironment\n",
    "from task2_agents import RuleBasedAgent, TemporalDifference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=10, starting_lane = 1):\n",
    "    all_episode_rewards = []\n",
    "    all_timesteps = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        \n",
    "        options = {\n",
    "            'starting_lane': starting_lane\n",
    "        }\n",
    "        state, _ = env.reset(options=options)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        cumulative_reward = 0\n",
    "        timestep = 0\n",
    "        action_mapping = {0: 'left', 1: 'stay', 2: 'right', 3: 'rest'}\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = agent.choose_action(state)\n",
    "            env.logger.info(f\"Action: {action_mapping[action]}\")\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            env.render()\n",
    "            cumulative_reward += reward\n",
    "            state = next_state\n",
    "            timestep += 1\n",
    "            \n",
    "            # Store rewards at each timestep for this episode\n",
    "            episode_rewards.append(cumulative_reward)\n",
    "\n",
    "            if truncated:\n",
    "                print('Episode terminated due to reaching maximum timesteps.')\n",
    "                break\n",
    "\n",
    "        # Append results for each episode\n",
    "        all_episode_rewards.append(cumulative_reward)\n",
    "        all_timesteps.append(timestep)\n",
    "\n",
    "    return all_episode_rewards, all_timesteps\n",
    "\n",
    "def plot_eval_metrics(all_episode_rewards, all_timesteps, window_size=10):\n",
    "    \"\"\"\n",
    "    Plot cumulative rewards and timesteps to termination with a rolling mean.\n",
    "\n",
    "    Args:\n",
    "    - all_episode_rewards (list of lists): Cumulative rewards for each episode.\n",
    "    - all_timesteps (list): Number of timesteps to termination for each episode.\n",
    "    - window_size (int): Window size for rolling mean.\n",
    "    \"\"\"\n",
    "    # Compute rolling mean for cumulative rewards across episodes\n",
    "    rolling_rewards = pd.DataFrame(all_episode_rewards).mean(axis=0).rolling(window_size).mean()\n",
    "    # Compute rolling mean for timesteps to termination\n",
    "    rolling_timesteps = pd.Series(all_timesteps).rolling(window_size).mean()\n",
    "\n",
    "    # Plot the cumulative rewards with rolling mean\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rolling_rewards, label='Cumulative Reward (Rolling Mean)')\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title(f'Cumulative Rewards Over Episodes (Rolling Mean - Window Size: {window_size})')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the timesteps to termination with rolling mean\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(rolling_timesteps, label='Timesteps to Termination (Rolling Mean)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Timesteps')\n",
    "    plt.title(f'Timesteps to Termination (Rolling Mean - Window Size: {window_size})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastest Adjacent Lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = CustomTrafficEnvironment(max_time_steps=10000, logging_level=logging.CRITICAL)\n",
    "\n",
    "# Evaluate each strategy\n",
    "strategy = 'fastest_adjacent'\n",
    "rewards_dict = {}\n",
    "timesteps_dict = {}\n",
    "\n",
    "env.logger.info(f\"Evaluating {strategy} strategy\")\n",
    "agent = RuleBasedAgent(strategy=strategy)\n",
    "# episode_rewards, checkpoint_rewards, timesteps_to_termination = evaluate_agent(agent, env)\n",
    "all_episode_rewards, all_timesteps = evaluate_agent(agent, env, num_episodes=100)\n",
    "rewards_dict['Fastest Adjacent Lane Agent'] = all_episode_rewards\n",
    "timesteps_dict['Fastest Adjacent Lane Agent'] = all_timesteps\n",
    "# env.logger.info(\"\\n\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative rewards for random strategy: 1048.1580000000001\n",
      "Average timesteps to termination for random strategy: 256.92\n"
     ]
    }
   ],
   "source": [
    "print(f'Average cumulative rewards for random strategy: {np.mean(rewards_dict['Fastest Adjacent Lane Agent'])}')\n",
    "print(f'Average timesteps to termination for random strategy: {np.mean(timesteps_dict[\"Fastest Adjacent Lane Agent\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = CustomTrafficEnvironment(max_time_steps=10000, logging_level=logging.CRITICAL)\n",
    "\n",
    "for i in range(5):\n",
    "    env.logger.info(f\"Evaluating Agent {i+1}\")\n",
    "    agent = RuleBasedAgent(strategy='stay')\n",
    "    all_episode_rewards, all_timesteps = evaluate_agent(agent, env, num_episodes=100, starting_lane=i+1)\n",
    "    rewards_dict[f'Same Lane Agent {i+1}'] = all_episode_rewards\n",
    "    timesteps_dict[f'Same Lane Agent {i+1}'] = all_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative rewards for Same Lane Agent 1: 202.07799999999986\n",
      "Average timesteps to termination for Same Lane Agent 1: 380.41\n",
      "Average cumulative rewards for Same Lane Agent 2: 270.2979999999999\n",
      "Average timesteps to termination for Same Lane Agent 2: 373.62\n",
      "Average cumulative rewards for Same Lane Agent 3: 209.644\n",
      "Average timesteps to termination for Same Lane Agent 3: 379.73\n",
      "Average cumulative rewards for Same Lane Agent 4: -10.907999999999998\n",
      "Average timesteps to termination for Same Lane Agent 4: 401.77\n",
      "Average cumulative rewards for Same Lane Agent 5: 263.44500000000005\n",
      "Average timesteps to termination for Same Lane Agent 5: 374.34\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'Average cumulative rewards for Same Lane Agent {i+1}: {np.mean(rewards_dict[f\"Same Lane Agent {i+1}\"])}')\n",
    "    print(f'Average timesteps to termination for Same Lane Agent {i+1}: {np.mean(timesteps_dict[f\"Same Lane Agent {i+1}\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value-based Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:51<00:57,  1.32s/it]"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = CustomTrafficEnvironment(max_time_steps=10000, logging_level=logging.CRITICAL)\n",
    "\n",
    "td_lambda = TemporalDifference(env, oiv = 0.1, alpha=0.01, epsilon=0.1, lambd=0.6, gamma=0.9)\n",
    "total_reward_list, total_steps_list = td_lambda.train(num_episodes = 100, on_policy = True, save_model = False, log=True)\n",
    "total_rew = sum(total_reward_list)\n",
    "avg_rew = np.mean(total_reward_list)\n",
    "avg_steps = np.mean(total_steps_list)\n",
    "\n",
    "print(f'Sum of total rewards = {total_rew}')\n",
    "print(f'Average total reward per episode = {avg_rew}')\n",
    "print(f'Average steps per episode = {avg_steps}')\n",
    "\n",
    "# plot training metrics\n",
    "td_lambda.plot_training_metrics(window_size=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5754\n"
     ]
    }
   ],
   "source": [
    "print(len(td_lambda.Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Space Size = 10 (Distance) x 5 (Lanes) x 2 (Fatigue) x 5 x 5 x 5 (Clearance Rates) x 4 (Actions) = 50,000  \n",
    "State Space Size = 10 (Distance) x 5 (Lanes) x 5 x 5 x 5 (Clearance Rates) x 3 (Actions) = 18,750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rewards per Episode: -135820.22000000003\n",
      "Average Steps per Episode: 8039.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent in inference mode\n",
    "all_rewards, all_steps, checkpoint_rewards = td_lambda.evaluate(num_episodes=10, checkpoint_interval=50)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Rewards per Episode: {np.mean(all_rewards)}\")\n",
    "print(f\"Average Steps per Episode: {np.mean(all_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
