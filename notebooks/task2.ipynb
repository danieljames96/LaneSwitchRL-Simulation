{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from environment_gym import TrafficEnvironment\n",
    "from task2_environment import CustomTrafficEnvironment\n",
    "from task2_agents import RuleBasedAgent, TemporalDifference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=10, starting_lane = 1):\n",
    "    all_episode_rewards = []\n",
    "    all_timesteps = []\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        episode_rewards = []\n",
    "        \n",
    "        options = {\n",
    "            'starting_lane': starting_lane\n",
    "        }\n",
    "        state, _ = env.reset(options=options)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        cumulative_reward = 0\n",
    "        timestep = 0\n",
    "        action_mapping = {0: 'left', 1: 'stay', 2: 'right'}\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = agent.choose_action(state)\n",
    "            env.logger.info(f\"Action: {action_mapping[action]}\")\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            env.render()\n",
    "            cumulative_reward += reward\n",
    "            state = next_state\n",
    "            timestep += 1\n",
    "            \n",
    "            # Store rewards at each timestep for this episode\n",
    "            episode_rewards.append(cumulative_reward)\n",
    "\n",
    "            if truncated:\n",
    "                print('Episode terminated due to reaching maximum timesteps.')\n",
    "                break\n",
    "\n",
    "        # Append results for each episode\n",
    "        all_episode_rewards.append(cumulative_reward)\n",
    "        all_timesteps.append(timestep)\n",
    "\n",
    "    return all_episode_rewards, all_timesteps\n",
    "\n",
    "def plot_eval_metrics(all_episode_rewards, all_timesteps, window_size=10):\n",
    "    \"\"\"\n",
    "    Plot cumulative rewards and timesteps to termination with a rolling mean.\n",
    "\n",
    "    Args:\n",
    "    - all_episode_rewards (list of lists): Cumulative rewards for each episode.\n",
    "    - all_timesteps (list): Number of timesteps to termination for each episode.\n",
    "    - window_size (int): Window size for rolling mean.\n",
    "    \"\"\"\n",
    "    # Compute rolling mean for cumulative rewards across episodes\n",
    "    rolling_rewards = pd.DataFrame(all_episode_rewards).mean(axis=0).rolling(window_size).mean()\n",
    "    # Compute rolling mean for timesteps to termination\n",
    "    rolling_timesteps = pd.Series(all_timesteps).rolling(window_size).mean()\n",
    "\n",
    "    # Plot the cumulative rewards with rolling mean\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rolling_rewards, label='Cumulative Reward (Rolling Mean)')\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title(f'Cumulative Rewards Over Episodes (Rolling Mean - Window Size: {window_size})')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the timesteps to termination with rolling mean\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(rolling_timesteps, label='Timesteps to Termination (Rolling Mean)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Timesteps')\n",
    "    plt.title(f'Timesteps to Termination (Rolling Mean - Window Size: {window_size})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastest Adjacent Lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:46<00:00, 10.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = CustomTrafficEnvironment(max_time_steps=10000, logging_level=logging.CRITICAL)\n",
    "\n",
    "# Evaluate each strategy\n",
    "strategy = 'fastest_adjacent'\n",
    "rewards_dict = {}\n",
    "timesteps_dict = {}\n",
    "\n",
    "env.logger.info(f\"Evaluating {strategy} strategy\")\n",
    "agent = RuleBasedAgent(strategy=strategy)\n",
    "# episode_rewards, checkpoint_rewards, timesteps_to_termination = evaluate_agent(agent, env)\n",
    "all_episode_rewards, all_timesteps = evaluate_agent(agent, env, num_episodes=500)\n",
    "rewards_dict['Fastest Adjacent Lane Agent'] = all_episode_rewards\n",
    "timesteps_dict['Fastest Adjacent Lane Agent'] = all_timesteps\n",
    "# env.logger.info(\"\\n\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative rewards for Fastest Adjacent Lane strategy: 919.327\n",
      "Average timesteps to termination for Fastest Adjacent Lane strategy: 266.958\n"
     ]
    }
   ],
   "source": [
    "print(f'Average cumulative rewards for Fastest Adjacent Lane strategy: {np.mean(rewards_dict['Fastest Adjacent Lane Agent'])}')\n",
    "print(f'Average timesteps to termination for Fastest Adjacent Lane strategy: {np.mean(timesteps_dict[\"Fastest Adjacent Lane Agent\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:11<00:00,  6.99it/s]\n",
      "100%|██████████| 500/500 [01:10<00:00,  7.14it/s]\n",
      "100%|██████████| 500/500 [01:08<00:00,  7.29it/s]\n",
      "100%|██████████| 500/500 [01:10<00:00,  7.10it/s]\n",
      "100%|██████████| 500/500 [01:11<00:00,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = CustomTrafficEnvironment(max_time_steps=10000, logging_level=logging.CRITICAL)\n",
    "\n",
    "for i in range(5):\n",
    "    env.logger.info(f\"Evaluating Agent {i+1}\")\n",
    "    agent = RuleBasedAgent(strategy='stay')\n",
    "    all_episode_rewards, all_timesteps = evaluate_agent(agent, env, num_episodes=500, starting_lane=i+1)\n",
    "    rewards_dict[f'Same Lane Agent {i+1}'] = all_episode_rewards\n",
    "    timesteps_dict[f'Same Lane Agent {i+1}'] = all_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative rewards for Same Lane Agent 1: 134.14960000000002\n",
      "Average timesteps to termination for Same Lane Agent 1: 387.258\n",
      "Average cumulative rewards for Same Lane Agent 2: 197.26200000000006\n",
      "Average timesteps to termination for Same Lane Agent 2: 380.932\n",
      "Average cumulative rewards for Same Lane Agent 3: 180.65\n",
      "Average timesteps to termination for Same Lane Agent 3: 382.484\n",
      "Average cumulative rewards for Same Lane Agent 4: 70.00560000000002\n",
      "Average timesteps to termination for Same Lane Agent 4: 393.568\n",
      "Average cumulative rewards for Same Lane Agent 5: 122.21100000000006\n",
      "Average timesteps to termination for Same Lane Agent 5: 388.488\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'Average cumulative rewards for Same Lane Agent {i+1}: {np.mean(rewards_dict[f\"Same Lane Agent {i+1}\"])}')\n",
    "    print(f'Average timesteps to termination for Same Lane Agent {i+1}: {np.mean(timesteps_dict[f\"Same Lane Agent {i+1}\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value-based Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 25/500 [00:04<01:29,  5.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m CustomTrafficEnvironment(max_time_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, logging_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m td_lambda \u001b[38;5;241m=\u001b[39m TemporalDifference(env, oiv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lambd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m total_reward_list, total_steps_list \u001b[38;5;241m=\u001b[39m \u001b[43mtd_lambda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_policy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m total_rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(total_reward_list)\n\u001b[0;32m      7\u001b[0m avg_rew \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(total_reward_list)\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive - Singapore Management University\\SMU Singapore\\Academics\\Aug 2024\\CS609_ReinforcementLearning\\Project\\LaneSwitchRL-Simulation\\notebooks\\../src\\task2_agents.py:127\u001b[0m, in \u001b[0;36mTemporalDifference.train\u001b[1;34m(self, num_episodes, on_policy, save_model, checkpoint_interval, log)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive - Singapore Management University\\SMU Singapore\\Academics\\Aug 2024\\CS609_ReinforcementLearning\\Project\\LaneSwitchRL-Simulation\\notebooks\\../src\\task2_environment.py:220\u001b[0m, in \u001b[0;36mCustomTrafficEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Handle lane change if not staying\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mapped_action \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 220\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attempt_lane_change\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapped_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# No action needed for staying in the current lane\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Update lane clearance rates based on neighboring lanes\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_clearance_rates()\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive - Singapore Management University\\SMU Singapore\\Academics\\Aug 2024\\CS609_ReinforcementLearning\\Project\\LaneSwitchRL-Simulation\\notebooks\\../src\\task2_environment.py:140\u001b[0m, in \u001b[0;36mCustomTrafficEnvironment._attempt_lane_change\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLane change failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLane change action out of bounds. Current lane: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_lane\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, New Lane: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnew_lane\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,  Action: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maction\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m penalty\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1551\u001b[0m, in \u001b[0;36mLogger.warning\u001b[1;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;124;03mLog 'msg % args' with severity 'WARNING'.\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;124;03mlogger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=True)\u001b[39;00m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(WARNING):\n\u001b[1;32m-> 1551\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWARNING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1684\u001b[0m, in \u001b[0;36mLogger._log\u001b[1;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001b[0m\n\u001b[0;32m   1681\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m   1682\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakeRecord(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, level, fn, lno, msg, args,\n\u001b[0;32m   1683\u001b[0m                          exc_info, func, extra, sinfo)\n\u001b[1;32m-> 1684\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1700\u001b[0m, in \u001b[0;36mLogger.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_record, LogRecord):\n\u001b[0;32m   1699\u001b[0m     record \u001b[38;5;241m=\u001b[39m maybe_record\n\u001b[1;32m-> 1700\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallHandlers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1762\u001b[0m, in \u001b[0;36mLogger.callHandlers\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1760\u001b[0m     found \u001b[38;5;241m=\u001b[39m found \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mlevelno \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m hdlr\u001b[38;5;241m.\u001b[39mlevel:\n\u001b[1;32m-> 1762\u001b[0m         \u001b[43mhdlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m c\u001b[38;5;241m.\u001b[39mpropagate:\n\u001b[0;32m   1764\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m#break out\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1028\u001b[0m, in \u001b[0;36mHandler.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1028\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1280\u001b[0m, in \u001b[0;36mFileHandler.emit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m-> 1280\u001b[0m     \u001b[43mStreamHandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1164\u001b[0m, in \u001b[0;36mStreamHandler.emit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;66;03m# issue 35046: merged two stream.writes into one.\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     stream\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminator)\n\u001b[1;32m-> 1164\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m:  \u001b[38;5;66;03m# See issue 36272\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py:1144\u001b[0m, in \u001b[0;36mStreamHandler.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1144\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = CustomTrafficEnvironment(max_time_steps=10000, logging_enabled=False)\n",
    "\n",
    "td_lambda = TemporalDifference(env, oiv = 0.1, alpha=0.1, epsilon=0.1, lambd=0.6, gamma=0.95)\n",
    "total_reward_list, total_steps_list = td_lambda.train(num_episodes = 500, on_policy = True, save_model = False)\n",
    "total_rew = sum(total_reward_list)\n",
    "avg_rew = np.mean(total_reward_list)\n",
    "avg_steps = np.mean(total_steps_list)\n",
    "\n",
    "print(f'Sum of total rewards = {total_rew}')\n",
    "print(f'Average total reward per episode = {avg_rew}')\n",
    "print(f'Average steps per episode = {avg_steps}')\n",
    "\n",
    "# plot training metrics\n",
    "td_lambda.plot_training_metrics(window_size=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "print(len(td_lambda.Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " State Space Size = 10 (Distance) x 5 (Lanes) x 5 x 5 x 5 (Clearance Rates) x 3 (Actions) = 18,750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_lambda.Env.enable_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative rewards for TD-Lambda Agent: -809.8999999999997\n",
      "Average timesteps to termination for TD-Lambda Agent: 451.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent in inference mode\n",
    "all_rewards, all_steps, checkpoint_rewards = td_lambda.evaluate(num_episodes=1, checkpoint_interval=50)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average cumulative rewards for TD-Lambda Agent: {np.mean(all_rewards)}\")\n",
    "print(f\"Average timesteps to termination for TD-Lambda Agent: {np.mean(all_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_lambda.Env.disable_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative rewards for TD-Lambda Agent: -481.6\n",
      "Average timesteps to termination for TD-Lambda Agent: 422.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average cumulative rewards for TD-Lambda Agent: {np.mean(all_rewards)}\")\n",
    "print(f\"Average timesteps to termination for TD-Lambda Agent: {np.mean(all_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based Agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
